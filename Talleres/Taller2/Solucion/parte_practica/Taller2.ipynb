{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller 2. Construcción explicita de Redes Neutonales\n",
    "\n",
    "Nombres: Manuel Sanchez y Allan Ramirez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio I\n",
    "(10 pts) Verifique de dónde vienen las ecuaciones fundamentales 3 y 4. Intente demostrarlo\n",
    "\n",
    "* PISTA: Escriba el input ponderado en términos de los pesos y los sesgos y encuentre las derivadas\n",
    "buscadas utilizando dichas ecuaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para demostrar de donde viene la ecuaciones 3 y 4, primero expresemos cuales son esas igualdades que queremos demostrar. \n",
    "\n",
    "Entonces la expresion 3 que es el gradiente de los pesos, es definida de la siguiente manera:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b_j^l} = \\delta_j^l$$\n",
    "\n",
    "Para intentar demostrar la expresion anterior, debemos recordar como se expresa la entrada ponderada expresada en terminos de los pesos y los sesgos la cual llamaremos $z_j^l$\n",
    "\n",
    "$$\n",
    "z_j^l = \\sum_{k} W_{jk}^l a_k^{l-1} + b_j^l\n",
    "$$\n",
    "\n",
    "Ahora queremos encontrar la derivada de la funcion de costo $C$ con respecto al sesgo $b_j^l$, es decir, $\\frac{\\partial C}{\\partial b_j^l}$:\n",
    "\n",
    "Se sabe que la función de Costo depende de las activacion de la ultima capa peroa si mismo sabemos que las activacion depende de las entradas ponderadas $z_j^l$, entonces se decide aplicar la Regla de cadena de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial b_j^l} = \\frac{\\partial C}{\\partial z_j^l} \\cdot \\frac{\\partial z_j^l}{\\partial b_j^l}\n",
    "$$\n",
    "\n",
    "primero derivemos la expresion $\\frac{\\partial z_j^l}{\\partial b_j^l}$, entonces recordemos que si $z_j^l = \\sum_{k} W_{jk}^l a_k^{l-1} + b_j^l$, sabemos que la suma de los pesos por las activaciones son constantes entonces al derivar con respecto a $b_j^l$ se hacen cero, luego solo quedaria la expresion $b_j^l$ y con respecto a ella misma entonces es igual a uno, es decir:\n",
    "\n",
    "$$\\frac{\\partial z_j^l}{\\partial b_j^l}=1$$\n",
    "\n",
    "entonces volviendo a la expresion de la regla de la cadena, tenemos que:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial b_j^l} = \\frac{\\partial C}{\\partial z_j^l} \\cdot \\frac{\\partial z_j^l}{\\partial b_j^l} = \\frac{\\partial C}{\\partial z_j^l} \\cdot 1 = \\frac{\\partial C}{\\partial z_j^l}\n",
    "$$\n",
    "\n",
    "y por definicion sabemos que la derivada de la funcion de costo con respecto al input, es el termino de error que usamos en backpropagation $\\frac{\\partial C}{\\partial z_j^l} = \\delta_j^l$, entonces \n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b_j^l} = \\frac{\\partial C}{\\partial z_j^l} = \\delta_j^l$$\n",
    "\n",
    "Ahora bien por otro la tenemos la expresion 4 que es el gradientes de los pesos, es definida de la siguiente manera:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w_{jk}^l} = a_k^{l-1} \\delta_j^l$$\n",
    "\n",
    "de la misma forma debemos aplicar la regla de la cadena si queremos saber cual es derivada parcial de la funcion de Costo con respecto a los pesos de la siguiente manera:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w_{jk}^l} = \\frac{\\partial C}{\\partial z_j^l} \\cdot \\frac{\\partial z_j^l}{\\partial w_{jk}^l} $$\n",
    "\n",
    "entonces derivemos primero, $\\frac{\\partial z_j^l}{\\partial w_{jk}^l}$, recordemos que \n",
    "\n",
    "$$ z_j^l = \\sum_{k} W_{jk}^l a_k^{l-1} + b_j^l = W_{jk}^l a_k^{l-1} + \\sum_{m \\neq k} W_{jk}^l a_k^{l-1} + b_j^l$$\n",
    "\n",
    "entonces si realizamos la derivada de esta expresion con respecto a $W_{jk}^l$ entonces tanto la sumatoria con $b_j^l$ se pueden ver como constantes y $W_{jk}^l$ al derivarse con respecto a ella es igual a 1, entonces la expresion queda como:\n",
    "\n",
    "$$\\frac{\\partial z_j^l}{\\partial w_{jk}^l} = 1 \\cdot a_k^{l-1} = a_k^{l-1}$$\n",
    "\n",
    "y como habiamos mencionado anteriormente la derivada parcial del costo con respecto a la entreda es el error en la neurona j de la capa l, entonces tenemos la expresion $ \\frac{\\partial C}{\\partial z_j^l} = \\delta_j^l$, finalemente:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w_{jk}^l} = \\frac{\\partial C}{\\partial z_j^l} \\cdot \\frac{\\partial z_j^l}{\\partial w_{jk}^l} =  \\delta_j^l \\cdot  a_k^{l-1}$$\n",
    "\n",
    "entonces podemos demostrar como es que llega a ambas expresiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio II\n",
    "(10 pts) Suponga que tiene una red con 4 capas (una capa de input, dos capas ocultas y una\n",
    "capa de salida) y con una sola neurona en cada capa. Trate de construir el gradiente con las\n",
    "derivadas de la funcion de costo con respecto a los pesos y los sesgos “manualmente” y verifique\n",
    "que muchos de los terminos que aparecen en las ecuaciones coinciden.\n",
    "\n",
    "Con esto en mente describamos como se expresa la red que estamos mostrando en este ejercicio.\n",
    "\n",
    "* Primera capa oculta\n",
    "$$\n",
    "z^2 = W^2 a^1 + b^2\n",
    "$$\n",
    "$$\n",
    "a^2 = f(z^2)\n",
    "$$\n",
    "\n",
    "* Segunda capa oculta\n",
    "\n",
    "$$\n",
    "z^3 = W^3 a^2 + b^3\n",
    "$$\n",
    "$$\n",
    "a^3 = f(z^3)\n",
    "$$\n",
    "\n",
    "* Capa de salida \n",
    "\n",
    "$$\n",
    "z^4 = W^4 a^3 + b^4\n",
    "$$\n",
    "$$\n",
    "a^4 = f(z^4)\n",
    "$$\n",
    "\n",
    "* Funcion de costo\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{2} (a^4 - y)^2\n",
    "$$\n",
    "\n",
    "\n",
    "Si realizamos BP, se intenta calcular la funcion de costo con respecto a la salida, luego respecto al peso $W^4$ y luego con respecto al sesgo $b^4$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z^4} = \\frac{\\partial C}{\\partial a^4} \\cdot \\frac{\\partial a^4}{\\partial z^4}\n",
    "$$\n",
    "\n",
    "donde \n",
    "\n",
    "$$\n",
    "\\frac{\\partial a^4}{\\partial z^4} = f'(z^4)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z^4} = (a^4 - y) f'(z^4)\n",
    "$$\n",
    "$$\n",
    "\\delta^4 = (a^4 - y) f'(z^4)\n",
    "$$\n",
    "\n",
    "entonces \n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial W^4} = \\delta^4 a^3\n",
    "$$\n",
    "\n",
    "y\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial b^4} = \\delta^4\n",
    "$$\n",
    "\n",
    "\n",
    "Ahora revisemos la segunda capa oculta, el error escribe de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\delta^3 = \\delta^4 W^4 f'(z^3)\n",
    "$$\n",
    "\n",
    "y el gradiente con respecto a $w^3$ y con respecto a $b^3$, es:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial W^3} = \\delta^3 a^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial b^3} = \\delta^3\n",
    "$$\n",
    "\n",
    "y el proceso para la primera capa oculta, tenemos que el error se expresa de la siguente manera:\n",
    "\n",
    "$$\n",
    "\\delta^2 = \\delta^3 W^3 f'(z^2)\n",
    "$$\n",
    "\n",
    "y calculando ambos gradientes con respecto $W^2$ y $b^2$ tenemos que:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial W^2} = \\delta^2 a^1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial b^2} = \\delta^2\n",
    "$$\n",
    "\n",
    "y entonces estamos demostrando que la expresiones del anterior punto funcionan de forma general, si quisieramos calcular los respectivos gradientes.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
